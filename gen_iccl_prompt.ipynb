{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on math subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|██████████| 378/378 [00:00<00:00, 14046.79 examples/s]\n",
      "Generating validation split: 100%|██████████| 41/41 [00:00<00:00, 27781.34 examples/s]\n",
      "Generating dev split: 100%|██████████| 5/5 [00:00<00:00, 4374.53 examples/s]\n",
      "Generating test split: 100%|██████████| 270/270 [00:00<00:00, 87293.77 examples/s]\n",
      "Generating validation split: 100%|██████████| 29/29 [00:00<00:00, 16428.26 examples/s]\n",
      "Generating dev split: 100%|██████████| 5/5 [00:00<00:00, 3339.95 examples/s]\n",
      "Generating test split: 100%|██████████| 100/100 [00:00<00:00, 28360.97 examples/s]\n",
      "Generating validation split: 100%|██████████| 11/11 [00:00<00:00, 4101.46 examples/s]\n",
      "Generating dev split: 100%|██████████| 5/5 [00:00<00:00, 2997.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "mmlu_math1 = load_dataset(\"cais/mmlu\", \"elementary_mathematics\")\n",
    "mmlu_math2 = load_dataset(\"cais/mmlu\", \"high_school_mathematics\")\n",
    "mmlu_math3 = load_dataset(\"cais/mmlu\", \"college_mathematics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is the value of p in 24 = 2p?', 'subject': 'elementary_mathematics', 'choices': ['p = 4', 'p = 8', 'p = 12', 'p = 24'], 'answer': 2}\n"
     ]
    }
   ],
   "source": [
    "# Check the dataset format\n",
    "print(mmlu_math1['test'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_iccl_single_example(example):\n",
    "    question = example['question'] + \"\\nChoose the best answer from the following options:\" + \"\\n\" + \"\\n\".join([f\"{i}. {option}\" for i, option in enumerate(example['choices'])]) + \"\\nAnswer: \"\n",
    "    return question, example['answer']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the value of p in 24 = 2p?\n",
      "Choose the best answer from the following options:\n",
      "0. p = 4\n",
      "1. p = 8\n",
      "2. p = 12\n",
      "3. p = 24\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(gen_iccl_single_example(mmlu_math1['test'][0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_all_iccl_prompts(easy_dataset, medium_dataset, hard_dataset, n_prompts, topic):\n",
    "    try:\n",
    "        with open(\"iccl_prompts.json\", 'r') as f:\n",
    "            iccl_prompts = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        iccl_prompts = {}\n",
    "    if topic not in iccl_prompts:\n",
    "        iccl_prompts[topic] = {}\n",
    "    \n",
    "    # Convert datasets to lists for easier shuffling & tracking\n",
    "    easy_examples = list(easy_dataset['test'])\n",
    "    medium_examples = list(medium_dataset['test'])\n",
    "    hard_examples = list(hard_dataset['test'])\n",
    "\n",
    "    # Use indices to track used hard examples\n",
    "    used_hard_indices = set()\n",
    "    prompts = []\n",
    "    for prompt_idx in range(n_prompts):\n",
    "        # Curriculum examples for demonstrations\n",
    "        demo_easy = random.choice(easy_examples)\n",
    "        demo_medium = random.choice(medium_examples)\n",
    "        demo_hard_index = random.choice([i for i in range(len(hard_examples)) if i not in used_hard_indices])\n",
    "        demo_hard = hard_examples[demo_hard_index]\n",
    "\n",
    "        # Can't use hard example that appeared in demonstration for testing\n",
    "        used_hard_indices.add(demo_hard_index)\n",
    "        available_hard_indices = [i for i in range(len(hard_examples)) if i not in used_hard_indices]\n",
    "        if not available_hard_indices:\n",
    "            raise ValueError(\"Not enough unique hard examples available\")\n",
    "        test_hard_index = random.choice(available_hard_indices)\n",
    "        test_hard = hard_examples[test_hard_index]\n",
    "        used_hard_indices.add(test_hard_index)\n",
    "\n",
    "        demonstrations = [\n",
    "            gen_iccl_single_example(demo_easy),\n",
    "            gen_iccl_single_example(demo_medium),\n",
    "            gen_iccl_single_example(demo_hard)\n",
    "        ]\n",
    "        test_question, test_answer = gen_iccl_single_example(test_hard)\n",
    "\n",
    "        prompt = \"\"\n",
    "        for demo_q, demo_a in demonstrations:\n",
    "            prompt += f\"{demo_q}\" + str(demo_a) + \"\\n\\n\"\n",
    "        prompt += f\"{test_question}\"\n",
    "\n",
    "        iccl_prompts[topic][f\"{prompt_idx}\"] = {\n",
    "            \"question\": prompt,\n",
    "            \"answer\": test_answer\n",
    "        }\n",
    "\n",
    "    # Write the updated JSON back to the file\n",
    "    with open(\"iccl_prompts.json\", 'w') as f:\n",
    "        json.dump(iccl_prompts, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_all_iccl_prompts(mmlu_math1, mmlu_math2, mmlu_math3, int(len(mmlu_math3['test']) // 2), \"math\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate ICCL prompts for all topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
